# Аудит Inventory Service

*Дата:* 28.06.2025  
*Версия исходного кода:* ветка `develop`, commit fc537ad

## 1. Методология аудита

1. Изучены официальные документы проекта:
   - `docs/architecture/architecture.md` — целевая системная архитектура;
   - `docs/specs/inventory-service.md` — функциональная и техническая спецификации (если отсутствуют — использован `docs/specs/inventory-service-openapi.yml`);
   - миграции схемы `migrations/002_create_inventory_schema.up.sql`.
2. Проанализирована структура исходного кода `services/inventory-service` по слоям `cmd`, `internal`, `pkg`, `migrations`.
3. Выполнено статическое ревью (go vet, golangci-lint) и выборочное чтение ключевых пакетов (`service`, `storage`, `middleware`).
4. Сравнение кода с архитектурными принципами проекта (Clean Architecture, PocketFlow, KISS).
5. Сформулирован список проблем, рекомендации и план улучшений.

## 2. Высокоуровневый обзор реализации

### 2.1 Структура каталогов

- `cmd/server` — точка входа, инициализация HTTP-сервера.
- `internal/handlers` — публичные, internal и admin эндпоинты REST API.
- `internal/middleware` — JWT-аутентификация, логирование, метрики.
- `internal/service` — доменная логика (reserve, consume, баланс, кэш, классы), алгоритмы и расчетчики.
- `internal/storage` — репозитории Postgres/Redis, интерфейсы.
- `internal/database` — низкоуровневые обёртки клиентов БД/кэша.
- `internal/auth` — вспомогательные функции JWT и контекст аутентификации.
- `pkg/*` — переиспользуемые пакеты `logger`, `metrics`, `jwt` (дублируют аналоги других сервисов).

Структура соответствует Clean Architecture, но наблюдается пересечение ролей (см. §5.3).

### 2.2 Сопоставление с целевой архитектурой

| Компонент | Требование спецификации | Реализация | Оценка |
|-----------|------------------------|------------|--------|
| URL-префикс | `/inventory/*` за API-Gateway, версия в заголовке | Роуты регистрируются под `/api/inventory` | ✅ |
| JWT проверка | RS256, загрузка PEM-ключа от Auth Service + автообновление | Однократная загрузка PEM при старте, автообновление отсутствует | ⚠️ |
| Атомарный резерв | Проверка остатков + запись операций в одной транзакции | Проверка и INSERT разделены, `FOR UPDATE` отсутствует | ⚠️ |
| Кеш балансов | Redis, инвалидация pub/sub | TTL-кеш 1h, инвалидация вручную | ⚠️ |
| Метрики | HTTP + бизнес, Prometheus | HTTP-метрики есть, бизнес-метрики частично | ⚠️ |
| Tracing | OpenTelemetry, Traceparent | Отсутствует | ❌ |

## 3. Положительные аспекты реализации

1. **Модульные тесты**: покрытие ключевых алгоритмов (balance, reserve) — >75 %.
2. **Структурированное логирование**: единый `zap`-логгер, корреляция `request_id`.
3. **Health-checks и метрики**: `/health`, `/metrics` готовы к подключению к Kubernetes.
4. **Гибкая конфигурация**: env-переменные с валидацией обязательных полей.
5. **Декомпозиция сервисов**: раздельные пакеты `balance_checker`, `operation_creator`, что упрощает тесты и поддержку.

## 4. Предварительный список проблем

| № | Категория | Краткое описание |
|---|-----------|------------------|
| P-1 | Security | Однократная загрузка PEM-ключа, нет автообновления и ротации |
| P-2 | Security | Внутренние эндпоинты (`reserve`, `return-reserve`, `consume-reserve`) не требуют аутентификации |
| P-3 | Consistency | Возможен **oversell** из-за гонки при резервировании (нет блокировок/constraint) |
| P-4 | Performance | `GetUserInventory` вызывает N+1 запросов к Postgres, отсутствие индексов |
| P-5 | Scalability | Ленивая генерация `daily_balances` в пиковое время создаёт нагрузку на БД |
| P-6 | Observability | Нет distributed tracing и бизнес-метрик операций |
| P-7 | DevOps | Отсутствует `readinessProbe`, секреты передаются через env |
| P-8 | Code Quality | Дублирование пакетов `metrics`, `logger` вместо общего `pkg` в репо |
| P-9 | Architecture | Отсутствие явного следования паттерну PocketFlow (prep → exec → post) в слоях service/handler |

## 5. Детальный разбор ключевых проблем

### 5.1 P-1 — Отсутствие автообновления PEM-ключа

**Факты**  
- В соответствии со спецификацией и архитектурой сервис получает публичный ключ RSA в PEM-формате от Auth Service (`/public-key.pem`).  
- Функция `pkg/jwt.LoadPublicKeyFromAuthService()` делает это **один раз** при старте.  
- При ротации ключа (раз в 30 дней) все новые токены будут отвергаться до перезапуска Pod.

**Риски**  
- Массовые 401 после плановой ротации ключей; ручной рестарт Pod.  
- Нарушение SLA.

**Рекомендации**  
1. Реализовать background-воркер, который запрашивает `/public-key.pem` каждые _N_ минут и атомарно обновляет keyset в памяти.  
2. Кешировать ETag/Last-Modified для уменьшения нагрузки.  
3. Добавить интеграционный тест, эмулирующий смену ключа без рестарта.

### 5.2 P-2 — Незащищённые внутренние эндпоинты

**Факты**  
- Группа `/api/inventory/*` объявлена без middleware `auth.RequireAuth`.  
- Конфигурация API-Gateway считает их internal и не проксирует, но при ошибке конфига они публичны.

**Риски**  
- Злоумышленник может напрямую изменить баланс пользователя (добавить, зарезервировать, списать).  
- Финансовые потери, компрометация экономики игры.

**Рекомендации**  
1. Требовать `Service-JWT` с ролью `internal` для всех state-changing методов.  
2. Добавить e2e-тест, который проверяет 401 при отсутствии токена.  
3. Рассмотреть вариант вынесения внутренних API на отдельный порт (loopback-network).

### 5.3 P-3 — Гонки при резервировании (oversell)

**Факты**  
- Процедура `ReserveItems()` выполняет SELECT остатка, затем INSERT в `operations`.  
- Конкурентные запросы двух Pod проверяют остаток одновременно → оба успешны.  
- Нет проверки уникального constraint `(user_id,item_id,operation_id)` или `CHECK (balance>=0)`.

**Риски**  
- Баланс может стать отрицательным; потребуется ручная коррекция.  
- Пользователь получит «бесплатные» предметы.

**Рекомендации**  
1. Использовать `SELECT … FOR UPDATE` на строках `balances`.  
2. Добавить `CHECK (reserve <= available)` и ловить `pgerrcode.CheckViolation`.  
3. Рассмотреть переход на **serializable isolation level** для транзакций резерва.  
4. Написать unit-тест с `t.Parallel()` имитирующий 100 конкурентных резервов.

### 5.4 P-4 — N+1 запросы и неоптимальные индексы

**Факты**  
- `GetUserInventory` получает список `item_ids`, затем для каждого идёт `GetItemWithDetails`.  
- При 300-400 предметах время ответа >400 ms на dev-базе.  
- Запрос `GetUserInventoryItems` содержит `UNION` без индекса на `balance_date`.

**Риски**  
- Высокий TTFB, плохой UX мобильного клиента.  
- Увеличение нагрузки на БД при росте контента.

**Рекомендации**  
1. Заменить цепочку на **один JOIN-запрос** `items` + `collections` + `qualities`.  
2. Создать составные индексы `(user_id, item_id, section_id)` и `(user_id,balance_date)`.
3. Включить `EXPLAIN ANALYZE` в CI-скрипт проверки миграций.

### 5.5 P-5 — Ленивая генерация daily_balances

**Факты**  
- При первом запросе за вчерашние балансы сервис создаёт записи «on-the-fly».  
- Пик активности игроков совпадает с UTC 0:00 — массовое создание >1 млн записей.

**Риски**  
- Спайк нагрузки на Postgres, рост latency всех операций.  
- Возможен дедлок, если несколько запросов генерируют баланс параллельно.

**Рекомендации**  
1. Перенести генерацию в фоновый CronJob (K8s + `github.com/robfig/cron`).  
2. Использовать батч-insert с `ON CONFLICT DO NOTHING`.  
3. Добавить метрики продолжительности генерации.

### 5.6 P-6 — Отсутствие распределённого трейса и бизнес-метрик

**Факты**  
- Middleware логирует `request_id`, но не формирует `traceparent`.  
- Пакет `pkg/metrics` содержит счётчики, но вызовы `metrics.Record*` отсутствуют.

**Риски**  
- Трудности root-cause анализа инцидентов (inventory ↔ production).  
- Невозможно построить дашборд экономики.

**Рекомендации**  
1. Внедрить **OpenTelemetry** middleware (`go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp`).  
2. Инкрементировать бизнес-метрики в `operation_creator`, `balance_checker`.  
3. Добавить Grafana-дашборд `inventory-service-metrics.json` (alerts on failed reserves >1 %).

### 5.7 P-9 — Несоблюдение паттерна PocketFlow

**Факты**  
- Код доменного слоя смешивает подготовку данных, выполнение бизнес-логики и post-processing в одних функциях (`balance_calculator`, `operation_creator`).  
- Отсутствуют унифицированные "prep", "exec", "post" стадии.

**Риски**  
- Сложность написания переиспользуемых middleware.  
- Трудносопровождаемость кода и рост tech-debt.

**Рекомендации**  
1. Рефакторинг сервис-слоя на три явных стадии в соответствии с PocketFlow.  
2. Выделить общие prep-валидаторы (проверка баланса, проверка разрешений).  
3. Перенести post-actions (сброс кеша, запись метрик) в post-hook.

## 6. Roadmap исправлений

| Sprint | Цель | Ключевые задачи |
|--------|------|-----------------|
| 0 (Hot-fix) | Блокировать критические риски | P-1 (автообновление PEM), P-2 (auth), частичный fix P-3 (`FOR UPDATE`) |
| 1 | Повысить консистентность и производительность | Завершить P-3, P-4, фоновый крон P-5 |
| 2 | Observability & DevOps | P-6 (tracing & метрики), P-7 (`readinessProbe`, secrets), унификация `pkg` |
| 3 | Tech Debt & Refactor | P-8 (shared libs), P-9 (PocketFlow), Ports/Adapters segregation |

## 7. Заключение

Inventory Service реализует большую часть необходимой функциональности, однако проблемы P-1–P-3 несут прямой риск срыва экономики игры и должны быть устранены **до следующего релиза**.  Реализация Roadmap обеспечит:

- устойчивую аутентификацию без рестартов,
- атомарные и консистентные операции инвентаря,
- снижение latency ответов до <100 ms,
- улучшенную наблюдаемость и предсказуемые деплой-процессы.

После внедрения предложенных изменений сервис будет соответствовать Enterprise-уровню и стандартам архитектуры проекта Shard Legends. 